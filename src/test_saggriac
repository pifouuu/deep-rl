import tensorflow as tf
import numpy as np
from gym.envs.registration import make
import argparse
import pprint as pp
from ddpg.logger import Logger
from ddpg.memory import SARSTMemory, EpisodicHerSARSTMemory
from ddpg.regionTree4 import TreeMemory
import datetime
from ddpg.networks import ActorNetwork, CriticNetwork
from ddpg.ddpgAgent import DDPG_agent
from ddpg.noise import OrnsteinUhlenbeckActionNoise
from gym.spaces import Box
from ddpg.util import load, boolean_flag
import json
import os
import time

from ddpg.regionTree4 import zones1

class zones():
    def __init__(self):

        self.zones = []
        self.samples_per_zone = []
        self.comp_per_zone = []
        self.zone_difficulties = []

    def compute_comp(self, goal):
        goal_zone = None
        i = 0
        while goal_zone is None:
            if self.zones[i].contains(goal):
                goal_zone = self.zones[i]
            i += 1
        n_samples = self.samples_per_zone[i - 1]

        if n_samples < self.zone_difficulties[i - 1]:
            comp = 0
        else:
            comp = np.min([1, (n_samples - self.zone_difficulties[i - 1]) / 1000])

        self.samples_per_zone[i - 1] += 1
        return comp

class zones1(zones):
    def __init__(self, maxlen, n_window, dims):
        super(zones1, self).__init__()
        zone1 = Region(np.array([-1.2, -0.07, -1.2, -0.07]), np.array([0.6, 0.07, -0.6, 0]), maxlen=maxlen, n_window=n_window, dims=dims)
        zone2 = Region(np.array([-1.2, -0.07, -0.6, -0.07]), np.array([0.6, 0.07, 0, 0]), maxlen=maxlen, n_window=n_window, dims=dims)
        zone3 = Region(np.array([-1.2, -0.07, 0, -0.07]), np.array([0.6, 0.07, 0.6, 0]),maxlen=maxlen, n_window=n_window, dims=dims)
        zone4 = Region(np.array([-1.2, -0.07, -1.2, 0]), np.array([0.6, 0.07, -0.6, 0.07]), maxlen=maxlen, n_window=n_window, dims=dims)
        zone5 = Region(np.array([-1.2, -0.07, -0.6, 0]), np.array([0.6, 0.07, 0, 0.07]), maxlen=maxlen, n_window=n_window, dims=dims)
        zone6 = Region(np.array([-1.2, -0.07, 0, 0]), np.array([0.6, 0.07, 0.6, 0.07]), maxlen=maxlen, n_window=n_window, dims=dims)
        self.zones = [zone1, zone2, zone3, zone4, zone5, zone6]
        self.samples_per_zone = 6 * [0]
        self.comp_per_zone = 6 * [0]
        self.zone_difficulties = [0, 200, 400, 800, 1000, 1200]

class zones2(zones):
    def __init__(self):
        super(zones2, self).__init__()
        zone1 = Region(np.array([-1.2, -0.07]), np.array([0.6, 0.07]))
        self.zones = [zone1]
        self.samples_per_zone = [0]
        self.comp_per_zone = [0]
        self.zone_difficulties = [200]

class zones3(zones):
    def __init__(self):
        super(zones3, self).__init__()
        zone1 = Region(np.array([-1.2]), np.array([0]))
        zone2 = Region(np.array([0]), np.array([0.6]))
        self.zones = [zone1, zone2]
        self.samples_per_zone = 2*[0]
        self.comp_per_zone = 2*[0]
        self.zone_difficulties = [200, 500]

def main(args):
    """Despite following the directives of https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development, fully reproducible results could not be obtained. See here : https://github.com/keras-team/keras/issues/2280 for any improvements"""

    params = [str(args['env']),
             str(args['memory']),
             str(args['strategy']),
             str(args['n_her_goals']),
             str(args['alpha']),
             str(args['n_split']),
             str(args['split_min']),
             str(args['n_window']),
             str(args['sigma']),
             str(args['train_freq']),
             str(args['nb_train_iter']),
             str(args['reward_type']),
             str(args['sampler']),
             str(args['n_cut']),
             str(args['n_points'])]


    now = datetime.datetime.now().strftime("%Y%m%d%H%M%S_%f")

    # Two loggers are defined to retrieve information by step or by episode. Only episodic information is displayed to stdout.
    log_dir = os.path.join(args['log_dir'], '_'.join(params), now)
    os.makedirs(log_dir, exist_ok=True)
    with open(os.path.join(log_dir, 'config.txt'), 'w') as config_file:
        config_file.write(json.dumps(args))
    logger_step = Logger(dir=os.path.join(log_dir,'log_steps'), format_strs=['stdout', 'json'])
    logger_episode = Logger(dir=os.path.join(log_dir,'log_episodes'), format_strs=['stdout', 'json'])

    # os.environ['PYTHONHASHSEED'] = '0'
    # if args['random_seed'] is not None:
    #     np.random.seed(int(args['random_seed']))
    #     rn.seed(int(args['random_seed']))
    #     tf.set_random_seed(int(args['random_seed']))

    # Make calls EnvRegistry.make, which builds the environment from its specs defined in gym.envs.init end then builds a timeLimit wrapper around the environment to set the max amount of steps to run
    train_env = make(args['env'])
    test_env = make(args['env'])
    # test_env = Monitor(test_env, directory=save_dir)

    # Wraps each environment in a goal_wrapper to override basic env methods and be able to access goal space properties, or modify the environment simulation according to sampled goals. The wrapper classes paths corresponding to each environment are defined in gym.envs.int
    if train_env.spec._goal_wrapper_entry_point is not None:
        wrapper_cls = load(train_env.spec._goal_wrapper_entry_point)
        train_env = wrapper_cls(train_env, args['reward_type'])
        test_env = wrapper_cls(test_env, args['reward_type'])

    #TODO integrate the choice of memory in environments specs in gym.env.init
    if args['memory'] == 'sarst':
        memory = SARSTMemory(train_env, limit=int(1e6))
    elif args['memory'] == 'hsarst':
        memory = EpisodicHerSARSTMemory(train_env, limit=int(1e6), strategy=args['strategy'],
                                        n_her_goals=int(args['n_her_goals']))
    else:
        raise Exception('No existing memory defined')

    low = np.concatenate([train_env.observation_space.low, train_env.goal_space.low])
    high = np.concatenate([train_env.observation_space.high, train_env.goal_space.high])
    state_space = Box(low, high)

    # Noise for the actor in vanilla ddpg
    actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(train_env.action_dim), sigma=float(args['sigma']))

    with tf.Session() as sess:

        if args['random_seed'] is not None:
            np.random.seed(int(args['random_seed']))
            tf.set_random_seed(int(args['random_seed']))
            train_env.seed(int(args['random_seed']))
            test_env.seed(int(args['random_seed']))

        actor = ActorNetwork(sess,
                             train_env.state_dim,
                             train_env.action_dim,
                             float(args['tau']),
                             float(args['actor_lr']))

        critic = CriticNetwork(sess,
                               train_env.state_dim,
                               train_env.action_dim,
                               float(args['gamma']),
                               float(args['tau']),
                               float(args['critic_lr']))

        memory = TreeMemory(state_space,
                            train_env.state_to_goal,
                            memory,
                            actor,
                            critic,
                            max_regions=int(args['n_cut']),
                            n_split=int(args['n_split']),
                            split_min=float(args['split_min']),
                            alpha=float(args['alpha']),
                            maxlen=int(args['n_points']),
                            n_window=int(args['n_window']),
                            render=args['render_memory'],
                            sampler=args['sampler'])

        zones = zones1(maxlen=int(args['n_points']), n_window=int(args['n_window']), dims=train_env.state_to_goal)

        for i in range(10000):
            goal = memory.sample_random()
            val = zones.compute_comp(goal)
            memory.insert((goal, val))
            if i % 100 == 0:
                print(i)
                memory.update_tree()
                memory.update_CP_tree()
                memory.update_display()
                # time.sleep()

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='provide arguments for DDPG agent')

    # base parameters
    parser.add_argument('--actor-lr', help='actor network learning rate', default=0.0001)
    parser.add_argument('--critic-lr', help='critic network learning rate', default=0.001)
    parser.add_argument('--gamma', help='discount factor for critic updates', default=0.99)
    parser.add_argument('--tau', help='soft target update parameter', default=0.001)
    parser.add_argument('--buffer-size', help='max size of the replay buffer', default=1000000)
    parser.add_argument('--minibatch-size', help='size of minibatch for minibatch-SGD', default=64)
    parser.add_argument('--random-seed', help='random seed for repeatability', default=8)
    boolean_flag(parser, 'render-test', default=False)
    boolean_flag(parser, 'render-train', default=True)
    boolean_flag(parser, 'render-memory', default=True)
    boolean_flag(parser, 'invert-grads', default=True)
    boolean_flag(parser, 'target-clip', default=True)

    parser.add_argument('--env', help='choose the gym env', default='CMCFull-v0')
    parser.add_argument('--memory', help='type of memory to use', default='sarst')
    parser.add_argument('--strategy', help='hindsight strategy: final, episode or future', default='final')
    parser.add_argument('--n-her-goals', default=4)
    parser.add_argument('--alpha', help='proportion of prioritized goal sampling', default=0)
    parser.add_argument('--n-split', help='number of split comparisons', default=10)
    parser.add_argument('--split-min', help='minimum cp difference to allow split', default=1)
    parser.add_argument('--n-window', help='length of running window used to compute cp', default=20)
    parser.add_argument('--sigma', help="amount of exploration", default=0.3)
    parser.add_argument('--train-freq', help='training frequency', default=1)
    parser.add_argument('--nb-train-iter', help='training iteration number', default=1)
    parser.add_argument('--reward-type', help='sparse, dense', default='sparse')
    parser.add_argument('--sampler', help='random, initial, prioritized', default='initial')
    parser.add_argument('--n-cut', help='number of regions in goal space', default=8)
    parser.add_argument('--n-points', help='number of points stored in region', default=100)

    parser.add_argument('--max-steps', help='max num of episodes to do while training', default=500000)
    parser.add_argument('--log-dir', help='directory for storing run info',
                        default='/home/pierre/PycharmProjects/deep-rl/log/local/')
    parser.add_argument('--resume-timestamp', help='directory to retrieve weights of actor and critic',
                        default=None)
    parser.add_argument('--resume-step', help='resume_step', default=None)
    parser.add_argument('--nb-test-steps', help='number of steps in the environment during evaluation', default=1000)
    parser.add_argument('--save-freq', help='saving models weights frequency', default=1000)
    parser.add_argument('--eval-freq', help='evaluating every n training steps', default=1000)





    args = vars(parser.parse_args())
    
    pp.pprint(args)

    main(args)
